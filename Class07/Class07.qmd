---
title: "Class 7: Machine Learning 1"
author: "Joel Kosareff"
format: gfm
---

In this class we will explore clustering and dimensionality reduction methods.

## K-means

Make up some input data where we know what the answer should be.

```{r}
tmp <- c(rnorm(30, -3), rnorm(30, 3))
x <- cbind(x = tmp, y = rev(tmp))
head(x)
```
Quick plot of x to see the two groups at -3,3 and +3,-3
```{r}
plot(x)
```

Use the `kmeans()` function setting k to 2 and nstart=20
```{r}
km <- kmeans(x, centers = 2, nstart = 20)
km
```

Inspect the results

> Q. How many points are in each cluster?

```{r}
km$size
```

> Q. What 'component' of your result object details
        Cluster Assignment/membership?
        Cluster Center
        
```{r}
km$cluster
km$centers
```

> Q. Plot x colored by the kmeans cluster assignment and add cluster centers as blue 

```{r}
plot(x, col=km$cluster)
points(km$centers, col = "blue", pch=15)
```

Play with kmeans and ask for different number of clusters
```{r}
km <- kmeans(x, centers = 4, nstart = 20)
plot(x, col=km$cluster)
points(km$centers, col = "blue", pch=16, cex=2)
```

# Hierarchical Clustering 

This is another very useful and widely employed clustering method which has the advantage over kmeans in that it can help reveal the something of the true grouping in your data.

The `hclust()` function wants a distance matrix as input. We can get this from the `dist()` function.
```{r}
d <- dist(x)
hc <- hclust(d)
hc
```


There is a plot method for hclust results

```{r}
plot(hc)
abline(h = 10, col ="red")
```

To get my cluster membership vector I need to "cut" my tree to yield sub-trees or branches with all the members of a given cluster residing on the same cut branch. The function to do this is `cutree()`

```{r}
grps <- cutree(hc, h=10)
grps
```


```{r}
plot(x, col=grps)
```


We can run `cutree()` with a number of clusters instead of height with the `k=` argument. This cuts the tree at the number of clusters we want instead of manually deciding the height. 
```{r}
cutree(hc, k = 4)
```


# Principal Component Analysis (PCA)


The base R function for PCA is called `prcomp()`

## PCA of UK Food Data

Import the data
```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url, row.names = 1)
View(x)
```



```{r}
dim(x)
```


```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```


```{r}
pairs(x, col=rainbow(10), pch=16)
```

Use the `prcomp()`PCA function
```{r}
pca <- prcomp(t(x))
summary(pca)
```
A "PCA Plot" (a.k.a "Score plot", PC1vsPC2 plot, etc.)
```{r}
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500), pch = 15)
text(pca$x[,1], pca$x[,2], colnames(x), col=c("orange","red","blue","darkgreen"))
```

```{r}
v <- round( pca$sdev^2/sum(pca$sdev^2) * 100 )
v
```

```{r}
z <- summary(pca)
z$importance
```
```{r}
barplot(v, xlab="Principal Component", ylab="Percent Variation")
```


```{r}
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )
```

```{r}
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,2], las=2 )
```

```{r}
biplot(pca)
```

